#!/usr/bin/env python3
"""
Minimal Discovery Integration Test
Tests discovery tools integration without full agent orchestrator to bypass dependency issues
"""

import asyncio
import logging
import sys
import os
from datetime import datetime
from typing import Dict, List, Any
import traceback

# Add the project root to the path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MinimalDiscoveryIntegrationTester:
    """Test discovery tools integration without full orchestrator"""
    
    def __init__(self):
        self.test_results = {}
        self.test_symbols = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'NVDA']
        
        # Create a mock config manager for tools that need it
        class MockConfigManager:
            def __init__(self):
                self.config = {}
            def get(self, key, default=None):
                return self.config.get(key, default)
            def get_config(self):
                return self.config
        
        self.MockConfigManager = MockConfigManager
        
    async def test_discovery_tool_imports(self) -> Dict[str, Any]:
        """Test that discovery tools can be imported successfully"""
        logger.info("Testing discovery tool imports...")
        
        discovery_tools = {
            'volume_spike_scanner': 'tools.volume_spike_scanner',
            'news_driven_discovery': 'tools.news_driven_discovery', 
            'technical_breakout_scanner': 'tools.technical_breakout_scanner',
            'earnings_calendar_monitor': 'tools.earnings_calendar_monitor',
            'sector_rotation_detector': 'tools.sector_rotation_detector'
        }
        
        import_results = {}
        successful_imports = []
        failed_imports = []
        
        for tool_name, module_path in discovery_tools.items():
            try:
                module = __import__(module_path, fromlist=[tool_name])
                # Try to get the main class from the module
                if hasattr(module, 'VolumeSpikeScanner'):
                    class_obj = getattr(module, 'VolumeSpikeScanner')
                elif hasattr(module, 'NewsDrivenDiscovery'):
                    class_obj = getattr(module, 'NewsDrivenDiscovery')
                elif hasattr(module, 'TechnicalBreakoutScanner'):
                    class_obj = getattr(module, 'TechnicalBreakoutScanner')
                elif hasattr(module, 'EarningsCalendarMonitor'):
                    class_obj = getattr(module, 'EarningsCalendarMonitor')
                elif hasattr(module, 'SectorRotationDetector'):
                    class_obj = getattr(module, 'SectorRotationDetector')
                else:
                    # Get first class-like object
                    class_obj = None
                    for attr_name in dir(module):
                        attr = getattr(module, attr_name)
                        if isinstance(attr, type) and not attr_name.startswith('_'):
                            class_obj = attr
                            break
                
                import_results[tool_name] = {
                    'status': 'success',
                    'module': module_path,
                    'main_class': class_obj.__name__ if class_obj else 'Unknown'
                }
                successful_imports.append(tool_name)
                
            except Exception as e:
                import_results[tool_name] = {
                    'status': 'error',
                    'module': module_path,
                    'error': str(e)
                }
                failed_imports.append(tool_name)
                logger.error(f"Failed to import {tool_name}: {e}")
        
        return {
            'status': 'success' if len(failed_imports) == 0 else 'partial',
            'successful_imports': successful_imports,
            'failed_imports': failed_imports,
            'import_details': import_results,
            'success_rate': f"{(len(successful_imports)/len(discovery_tools))*100:.1f}%"
        }
    
    async def test_discovery_tool_initialization(self) -> Dict[str, Any]:
        """Test that discovery tools can be initialized"""
        logger.info("Testing discovery tool initialization...")
        
        initialization_results = {}
        successful_inits = []
        failed_inits = []
        
        mock_config = self.MockConfigManager()
        
        # Test Volume Spike Scanner
        try:
            from tools.volume_spike_scanner import VolumeSpikeScanner
            # Try with and without config manager
            try:
                scanner = VolumeSpikeScanner()
            except TypeError:
                scanner = VolumeSpikeScanner(mock_config)
            
            initialization_results['volume_spike_scanner'] = {
                'status': 'success',
                'class_name': 'VolumeSpikeScanner',
                'instance_created': True
            }
            successful_inits.append('volume_spike_scanner')
        except Exception as e:
            initialization_results['volume_spike_scanner'] = {
                'status': 'error',
                'error': str(e)
            }
            failed_inits.append('volume_spike_scanner')
        
        # Test News Driven Discovery
        try:
            from tools.news_driven_discovery import NewsDrivenDiscovery
            # Try with and without config manager
            try:
                discovery = NewsDrivenDiscovery()
            except TypeError:
                discovery = NewsDrivenDiscovery(mock_config)
            
            initialization_results['news_driven_discovery'] = {
                'status': 'success',
                'class_name': 'NewsDrivenDiscovery',
                'instance_created': True
            }
            successful_inits.append('news_driven_discovery')
        except Exception as e:
            initialization_results['news_driven_discovery'] = {
                'status': 'error',
                'error': str(e)
            }
            failed_inits.append('news_driven_discovery')
        
        # Test Technical Breakout Scanner
        try:
            from tools.technical_breakout_scanner import TechnicalBreakoutScanner
            # Try with and without config manager
            try:
                scanner = TechnicalBreakoutScanner()
            except TypeError:
                scanner = TechnicalBreakoutScanner(mock_config)
            
            initialization_results['technical_breakout_scanner'] = {
                'status': 'success',
                'class_name': 'TechnicalBreakoutScanner',
                'instance_created': True
            }
            successful_inits.append('technical_breakout_scanner')
        except Exception as e:
            initialization_results['technical_breakout_scanner'] = {
                'status': 'error',
                'error': str(e)
            }
            failed_inits.append('technical_breakout_scanner')
        
        # Test Earnings Calendar Monitor
        try:
            from tools.earnings_calendar_monitor import EarningsCalendarMonitor
            # Try with and without config manager
            try:
                monitor = EarningsCalendarMonitor()
            except TypeError:
                monitor = EarningsCalendarMonitor(mock_config)
            
            initialization_results['earnings_calendar_monitor'] = {
                'status': 'success',
                'class_name': 'EarningsCalendarMonitor',
                'instance_created': True
            }
            successful_inits.append('earnings_calendar_monitor')
        except Exception as e:
            initialization_results['earnings_calendar_monitor'] = {
                'status': 'error',
                'error': str(e)
            }
            failed_inits.append('earnings_calendar_monitor')
        
        # Test Sector Rotation Detector
        try:
            from tools.sector_rotation_detector import SectorRotationDetector
            # Try with and without config manager
            try:
                detector = SectorRotationDetector()
            except TypeError:
                detector = SectorRotationDetector(mock_config)
            
            initialization_results['sector_rotation_detector'] = {
                'status': 'success',
                'class_name': 'SectorRotationDetector',
                'instance_created': True
            }
            successful_inits.append('sector_rotation_detector')
        except Exception as e:
            initialization_results['sector_rotation_detector'] = {
                'status': 'error',
                'error': str(e)
            }
            failed_inits.append('sector_rotation_detector')
        
        return {
            'status': 'success' if len(failed_inits) == 0 else 'partial',
            'successful_initializations': successful_inits,
            'failed_initializations': failed_inits,
            'initialization_details': initialization_results,
            'success_rate': f"{(len(successful_inits)/5)*100:.1f}%"
        }
    
    async def test_discovery_tool_methods(self) -> Dict[str, Any]:
        """Test that discovery tools have expected methods"""
        logger.info("Testing discovery tool methods...")
        
        method_results = {}
        
        # Mock config manager for testing
        mock_config = self.MockConfigManager()
        
        # Test Volume Spike Scanner methods
        try:
            from tools.volume_spike_scanner import VolumeSpikeScanner
            # Try with and without config manager
            try:
                scanner = VolumeSpikeScanner()
            except TypeError:
                scanner = VolumeSpikeScanner(mock_config)
            
            expected_methods = ['scan_volume_spikes', 'analyze_volume_pattern', 'get_volume_data']
            available_methods = [method for method in expected_methods if hasattr(scanner, method)]
            
            method_results['volume_spike_scanner'] = {
                'status': 'success',
                'expected_methods': expected_methods,
                'available_methods': available_methods,
                'method_coverage': f"{(len(available_methods)/len(expected_methods))*100:.1f}%"
            }
        except Exception as e:
            method_results['volume_spike_scanner'] = {
                'status': 'error',
                'error': str(e)
            }
        
        # Test News Driven Discovery methods
        try:
            from tools.news_driven_discovery import NewsDrivenDiscovery
            # Try with and without config manager
            try:
                discovery = NewsDrivenDiscovery()
            except TypeError:
                discovery = NewsDrivenDiscovery(mock_config)
            
            expected_methods = ['discover_news_opportunities', 'analyze_news_sentiment', 'get_news_data']
            available_methods = [method for method in expected_methods if hasattr(discovery, method)]
            
            method_results['news_driven_discovery'] = {
                'status': 'success',
                'expected_methods': expected_methods,
                'available_methods': available_methods,
                'method_coverage': f"{(len(available_methods)/len(expected_methods))*100:.1f}%"
            }
        except Exception as e:
            method_results['news_driven_discovery'] = {
                'status': 'error',
                'error': str(e)
            }
        
        # Test Technical Breakout Scanner methods
        try:
            from tools.technical_breakout_scanner import TechnicalBreakoutScanner
            # Try with and without config manager
            try:
                scanner = TechnicalBreakoutScanner()
            except TypeError:
                scanner = TechnicalBreakoutScanner(mock_config)
            
            expected_methods = ['scan_breakouts', 'analyze_technical_patterns', 'calculate_indicators']
            available_methods = [method for method in expected_methods if hasattr(scanner, method)]
            
            method_results['technical_breakout_scanner'] = {
                'status': 'success',
                'expected_methods': expected_methods,
                'available_methods': available_methods,
                'method_coverage': f"{(len(available_methods)/len(expected_methods))*100:.1f}%"
            }
        except Exception as e:
            method_results['technical_breakout_scanner'] = {
                'status': 'error',
                'error': str(e)
            }
        
        # Test Earnings Calendar Monitor methods
        try:
            from tools.earnings_calendar_monitor import EarningsCalendarMonitor
            # Try with and without config manager
            try:
                monitor = EarningsCalendarMonitor()
            except TypeError:
                monitor = EarningsCalendarMonitor(mock_config)
            
            expected_methods = ['monitor_earnings_calendar', 'get_upcoming_earnings', 'analyze_earnings_impact']
            available_methods = [method for method in expected_methods if hasattr(monitor, method)]
            
            method_results['earnings_calendar_monitor'] = {
                'status': 'success',
                'expected_methods': expected_methods,
                'available_methods': available_methods,
                'method_coverage': f"{(len(available_methods)/len(expected_methods))*100:.1f}%"
            }
        except Exception as e:
            method_results['earnings_calendar_monitor'] = {
                'status': 'error',
                'error': str(e)
            }
        
        # Test Sector Rotation Detector methods
        try:
            from tools.sector_rotation_detector import SectorRotationDetector
            # Try with and without config manager
            try:
                detector = SectorRotationDetector()
            except TypeError:
                detector = SectorRotationDetector(mock_config)
            
            expected_methods = ['detect_sector_rotation', 'analyze_sector_performance', 'calculate_relative_strength']
            available_methods = [method for method in expected_methods if hasattr(detector, method)]
            
            method_results['sector_rotation_detector'] = {
                'status': 'success',
                'expected_methods': expected_methods,
                'available_methods': available_methods,
                'method_coverage': f"{(len(available_methods)/len(expected_methods))*100:.1f}%"
            }
        except Exception as e:
            method_results['sector_rotation_detector'] = {
                'status': 'error',
                'error': str(e)
            }
        
        successful_tests = len([r for r in method_results.values() if r['status'] == 'success'])
        
        return {
            'status': 'success' if successful_tests == 5 else 'partial',
            'method_test_results': method_results,
            'successful_method_tests': successful_tests,
            'total_tools_tested': 5,
            'success_rate': f"{(successful_tests/5)*100:.1f}%"
        }
    
    async def test_discovery_tool_execution(self) -> Dict[str, Any]:
        """Test basic execution of discovery tools"""
        logger.info("Testing discovery tool execution...")
        
        execution_results = {}
        
        # Create mock config for tools that need it
        mock_config = self.MockConfigManager()
        
        # Test Volume Spike Scanner execution
        try:
            from tools.volume_spike_scanner import VolumeSpikeScanner
            # Try with and without config manager
            try:
                scanner = VolumeSpikeScanner()
            except TypeError:
                scanner = VolumeSpikeScanner(mock_config)
            
            # Try to execute main method with minimal parameters
            if hasattr(scanner, 'scan_volume_spikes'):
                # This might fail due to missing config, but we test the method exists
                execution_results['volume_spike_scanner'] = {
                    'status': 'method_available',
                    'main_method': 'scan_volume_spikes',
                    'executable': True
                }
            else:
                execution_results['volume_spike_scanner'] = {
                    'status': 'method_missing',
                    'main_method': 'scan_volume_spikes',
                    'executable': False
                }
        except Exception as e:
            execution_results['volume_spike_scanner'] = {
                'status': 'error',
                'error': str(e)
            }
        
        # Test other tools similarly...
        tools_to_test = [
            ('news_driven_discovery', 'NewsDrivenDiscovery', 'discover_news_opportunities'),
            ('technical_breakout_scanner', 'TechnicalBreakoutScanner', 'scan_breakouts'),
            ('earnings_calendar_monitor', 'EarningsCalendarMonitor', 'monitor_earnings_calendar'),
            ('sector_rotation_detector', 'SectorRotationDetector', 'detect_sector_rotation')
        ]
        
        for tool_name, class_name, method_name in tools_to_test:
            try:
                module = __import__(f'tools.{tool_name}', fromlist=[class_name])
                tool_class = getattr(module, class_name)
                
                # Try with and without config manager
                try:
                    tool_instance = tool_class()
                except TypeError:
                    tool_instance = tool_class(mock_config)
                
                if hasattr(tool_instance, method_name):
                    execution_results[tool_name] = {
                        'status': 'method_available',
                        'main_method': method_name,
                        'executable': True
                    }
                else:
                    execution_results[tool_name] = {
                        'status': 'method_missing',
                        'main_method': method_name,
                        'executable': False
                    }
            except Exception as e:
                execution_results[tool_name] = {
                    'status': 'error',
                    'error': str(e)
                }
        
        successful_executions = len([r for r in execution_results.values() 
                                   if r['status'] in ['method_available', 'method_missing']])
        
        return {
            'status': 'success' if successful_executions == 5 else 'partial',
            'execution_test_results': execution_results,
            'successful_execution_tests': successful_executions,
            'total_tools_tested': 5,
            'success_rate': f"{(successful_executions/5)*100:.1f}%"
        }
    
    async def test_core_infrastructure_availability(self) -> Dict[str, Any]:
        """Test that core infrastructure components are available"""
        logger.info("Testing core infrastructure availability...")
        
        infrastructure_results = {}
        
        # Test core modules
        core_modules = [
            'core.config_manager',
            'utils.cache_manager',
            'utils.notifications',
            'data.unified_data_manager',
            'risk.risk_manager',
            'analytics.performance_analyzer'
        ]
        
        available_modules = []
        missing_modules = []
        
        for module_name in core_modules:
            try:
                module = __import__(module_name, fromlist=[''])
                available_modules.append(module_name)
                infrastructure_results[module_name] = {
                    'status': 'available',
                    'module_path': module.__file__ if hasattr(module, '__file__') else 'built-in'
                }
            except Exception as e:
                missing_modules.append(module_name)
                infrastructure_results[module_name] = {
                    'status': 'missing',
                    'error': str(e)
                }
        
        return {
            'status': 'success' if len(missing_modules) == 0 else 'partial',
            'available_modules': available_modules,
            'missing_modules': missing_modules,
            'infrastructure_details': infrastructure_results,
            'success_rate': f"{(len(available_modules)/len(core_modules))*100:.1f}%"
        }
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """Run all minimal integration tests"""
        logger.info("Starting minimal discovery integration tests...")
        
        test_suite = {
            'discovery_tool_imports': self.test_discovery_tool_imports,
            'discovery_tool_initialization': self.test_discovery_tool_initialization,
            'discovery_tool_methods': self.test_discovery_tool_methods,
            'discovery_tool_execution': self.test_discovery_tool_execution,
            'core_infrastructure_availability': self.test_core_infrastructure_availability
        }
        
        results = {}
        start_time = datetime.now()
        
        for test_name, test_func in test_suite.items():
            logger.info(f"Running test: {test_name}")
            test_start = datetime.now()
            
            try:
                result = await test_func()
                result['execution_time'] = (datetime.now() - test_start).total_seconds()
                results[test_name] = result
                
                if result['status'] == 'success':
                    logger.info(f"âœ… {test_name}: PASSED")
                elif result['status'] == 'partial':
                    logger.warning(f"âš ï¸ {test_name}: PARTIAL - Some issues detected")
                else:
                    logger.error(f"âŒ {test_name}: FAILED - {result.get('error', 'Unknown error')}")
                    
            except Exception as e:
                logger.error(f"âŒ {test_name}: CRASHED - {e}")
                results[test_name] = {
                    'status': 'crashed',
                    'error': str(e),
                    'traceback': traceback.format_exc(),
                    'execution_time': (datetime.now() - test_start).total_seconds()
                }
        
        total_time = (datetime.now() - start_time).total_seconds()
        
        # Generate summary
        total_tests = len(test_suite)
        passed_tests = len([r for r in results.values() if r['status'] == 'success'])
        partial_tests = len([r for r in results.values() if r['status'] == 'partial'])
        failed_tests = len([r for r in results.values() if r['status'] in ['error', 'crashed']])
        
        summary = {
            'test_summary': {
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'partial_tests': partial_tests,
                'failed_tests': failed_tests,
                'success_rate': f"{(passed_tests/total_tests)*100:.1f}%",
                'total_execution_time': f"{total_time:.2f}s"
            },
            'test_results': results,
            'timestamp': datetime.now().isoformat(),
            'test_symbols': self.test_symbols
        }
        
        return summary
    
    def print_report(self, results: Dict[str, Any]):
        """Print a formatted minimal integration test report"""
        print("\n" + "="*80)
        print("MINIMAL DISCOVERY INTEGRATION - TEST REPORT")
        print("="*80)
        
        summary = results['test_summary']
        print(f"\nğŸ“Š TEST SUMMARY:")
        print(f"   Total Tests: {summary['total_tests']}")
        print(f"   Passed: {summary['passed_tests']} âœ…")
        print(f"   Partial: {summary['partial_tests']} âš ï¸")
        print(f"   Failed: {summary['failed_tests']} âŒ")
        print(f"   Success Rate: {summary['success_rate']}")
        print(f"   Total Time: {summary['total_execution_time']}")
        
        print(f"\nğŸ” DETAILED RESULTS:")
        for test_name, result in results['test_results'].items():
            if result['status'] == 'success':
                status_icon = "âœ…"
            elif result['status'] == 'partial':
                status_icon = "âš ï¸"
            else:
                status_icon = "âŒ"
                
            print(f"\n   {status_icon} {test_name.replace('_', ' ').title()}")
            print(f"      Status: {result['status']}")
            print(f"      Time: {result.get('execution_time', 0):.2f}s")
            
            if result['status'] in ['success', 'partial']:
                if 'success_rate' in result:
                    print(f"      Success Rate: {result['success_rate']}")
                
                if 'successful_imports' in result:
                    print(f"      Successful Imports: {len(result['successful_imports'])}")
                    if result['failed_imports']:
                        print(f"      Failed Imports: {result['failed_imports']}")
                
                if 'successful_initializations' in result:
                    print(f"      Successful Initializations: {len(result['successful_initializations'])}")
                    if result['failed_initializations']:
                        print(f"      Failed Initializations: {result['failed_initializations']}")
                
                if 'available_modules' in result:
                    print(f"      Available Modules: {len(result['available_modules'])}")
                    if result['missing_modules']:
                        print(f"      Missing Modules: {result['missing_modules']}")
                        
            else:
                print(f"      Error: {result.get('error', 'Unknown')}")
        
        print(f"\nğŸ“ˆ INTEGRATION HEALTH:")
        if summary['passed_tests'] == summary['total_tests']:
            print("   ğŸŸ¢ Discovery tools fully integrated and functional!")
        elif summary['passed_tests'] + summary['partial_tests'] >= summary['total_tests'] * 0.8:
            print("   ğŸŸ¡ Most discovery tools working - Minor issues detected")
        else:
            print("   ğŸ”´ Discovery integration issues detected - Requires attention")
        
        print("\n" + "="*80)

async def main():
    """Main test execution"""
    tester = MinimalDiscoveryIntegrationTester()
    
    try:
        results = await tester.run_all_tests()
        tester.print_report(results)
        
        # Save results to file
        import json
        with open('minimal_discovery_integration_test_results.json', 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        print(f"\nğŸ’¾ Detailed results saved to: minimal_discovery_integration_test_results.json")
        
        # Return appropriate exit code
        if results['test_summary']['failed_tests'] == 0:
            return 0
        else:
            return 1
            
    except Exception as e:
        logger.error(f"Minimal integration test execution failed: {e}")
        print(f"\nâŒ CRITICAL ERROR: {e}")
        traceback.print_exc()
        return 2

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)